% !TEX root =  ../report.tex
\section{Methods}

The members of Group 43 were given a mock version of our user interface. The mocks were pictures of the designs of virtually all of the screens in our application. Above each mock was a title of what that screen is supposed to do. The mocks were also numbered and there were descriptions below each of them. The descriptions of each mock described how the user interface would function if it was to be implemented, mentioning what each button does, i.e. what screen it redirects to, or how it changes the state of the application.

The evaluators were to evaluate the designs that we had come up with. They were to do so individually and with no supervision from us. They were instructed to go through each screen at least twice and to try to follow to flow between the different screens. They were to report on any problems that they encountered pertaining to any of 10 heuristics. They were to identify each individual problem and write down each problem according to a template, that includes what the problem is, which heuristic it corresponds to, what difficulties the user might experience due to the problem, what the context for each problem is, and what the likely cause of each problem is. They were to each send the evaluation of the problems that they encountered, so we can compile and analyze them, their frequency, and their severity.

In the end, five people from Group 43 submitted their evaluations. This number of evaluators is right in line with suggested best practices for Heuristic Evaluation Reports. The evaluations were all about a page or two long, they included anywhere between 10-25 problems. The evaluations did all include the heuristic that each problem corresponded to.